%\section{Design and Implementation}
%\label{sec:groupDesign}
%In this section we will describe the the system design and implementation of group based peer assisted video streaming service. 

\subsection{Coalition Formation}
In our work, we want to form coalitions with \textit{``similar"} players so that they can divide and share the video segments among themselves to collectively download the video content. We define similarity based on maximum video quality a player can play. To form a coalition, we utilize following information -- appropriate quality for the player based on its viewport size and resolution, the expected throughput. The viewport resolution and size are easily available from the player. To estimate the expected throughput, a player $P_i$ wait for some time to download $\mathcal{T}$ number of video segments by itself and measure the throughput $\tau_i$ based on the amount of data downloaded per second. $\mathcal{T}$ is called \textit{Coalition Formation Threshold}, as a player needs to download these many segments directly from the streaming server before it can join a coalition. Once $\tau_i$ is determined, player $P_i$ set the target quality $\mathcal{Q}_{p_i}$ based on any existing ABR algorithm. Then the player $P_i$ tries to find a coalition $G_p$ in the vicinity (based on the information received through the \textit{Proximity Server}) such that $\mathcal{Q}_g \approx \mathcal{Q}_{p_i}$ and $\forall_{P_k \in G_p} delay(P_i, P_k) < t_d$ where $t_d$ is a threshold on the permissible delay between two coalition members (indicates network-proximity between two users). If there is no such coalition exists in the vicinity, the player continue as a standalone player until another player join with it to form a coalition.
%
%\begin{algorithm}[h]
%	Start\;
%	$\tau_i = 0$\;
%	$z = 0$\;
%	\While{True}{
%		WaitForNextRequest()\;
%		\eIf{Group formed?}{
%			waitToRecvSegFromGroup()\;
%			sendSegmentToPlayer()\;
%		}{
%			downloadBasic()\;
%			sendSegmentToPlayer()\;
%			$z = z + 1$\;
%			\If{$z \ge \mathcal{T}$}{
%				$\tau_i \leftarrow$ measureThroughput()\;
%				formGroup($\tau_i$)\;
%			}
%		}
%	}
%	\caption{Group formation algorithm}
%	\label{algo:groupFormation}
%\end{algorithm}
%\notesc{segment and chunk -- both the terms has been used -- use only ``segment".}
In our design, every player in a coalition connects to each-other based on the network-proximity and a delay bound, therefore, communication between two such players is expected to be fast enough and does not affect the Internet speed. We can find this type of scenario when multiple users share a common Internet backbone or in case of the Internet on cable where ISP connects multiple customers via a local area network. Although all the customers are in the same network, everyone has a different subscription plan. With these assumptions, we keep target video bitrate $\mathcal{Q}_g$ around $\min(2\times\tau_i, V_i)$, where $V_i$ is the maximum quality supported by the viewport. 
%It will give enough time to download a video segment with higher bitrate than its bandwidth.

After the coalition formation phase, we need to distribute the responsibility of segment downloading among the coalition members. Here each player collaboratively tries to increase the overall QoE of the coalition. However, it is crucial that the players in a coalition stay in playback sync, i.e. every player in a coalition needs to play approximately the same video segment. Whenever a new player joins a coalition, it matches its playback time as per the playback time of the coalition. In a steady state, every player stays in sync as all of them gets a video segment almost at the same time.

\subsection{Segment Download and Distribution}
Every player has a different network subscription, and therefore, the network quality changes differently. We create a coalition based on the observation of the initial network quality during the video start-up. However, the network quality over the time may change drastically from the initial video start-up phase. We set our goal in a way so that a player with a better network quality over time should not suffer due to a player which experiences low network quality during the playback after joining a coalition. Also, we do not want a player to be a free rider; every player has to contribute to the coalition. 
%Therefore, we define a fairness metric which is a function of the total download time to download the segment directly from the streaming server and the part of it that individual players download. 
%\notesc{The fairness need to be pointed out -- may not be here, but at the proper place, how this is ensured. This function has never been mentioned later. At what stage this is handled, and how.} \noteam{I am trying to find out}
So, in \textit{CoaliDASH}, a coalition need to collectively choose two different objectives -- i) which player will download the next segment, and ii) what will be the quality for the next segment. We use a leader based system to compute these decisions. In our algorithm, the leader downloads the current segment. It takes two decisions -- i) the bitrate for the current segment and ii) the next player (the next leader) to download the next segment.
The next leader selection should be done as soon as possible, however, it can decide the quality level just before sending the request to the the server.
%We call this leadership process.

\begin{algorithm}
	\DontPrintSemicolon
	\caption{\label{algo:leadership}$LeaderShip()$ -- Schedule collaborative segment downloads by coalition members}
	\KwIn{$P_i, S_i$}
	
	\While{$isNotAvailable(S_i)$ }{
		sleep($\delta$) \hfill // \texttt{Wait until the segment}\;
		\hfill // \texttt{is available in the server}
	}

	\While{$noBufferAvailable(P_i)$}{
		$sleep(\delta)$ \hfill // \texttt{Wait until buffer is} \;
		\hfill // \texttt{available in the player}
	}
	
	$P_{i+1} \leftarrow findNextLeader()$\;
	$LeaderShip(P_{i+1}, S_{i+1})$ \hfill // \texttt{Broadcast} \;
	$\mathcal{Q}_i \leftarrow findCurrentQuality()$\;
	$DownloadAndDistribute(P_i, S_i, \mathcal{Q}_i)$\;
\end{algorithm}

Algorithm~\ref{algo:leadership} summarizes the process. 
%\notesc{Give comments in the algorithms, so that they become understandable.} 
Here the routine $LeaderShip(P_i, S_i)$ informs every players in the coalition that the player $P_i$ is the leader for the segment $S_i$. 
%\notesc{What is nonblocking RPC here -- you need to explain. If it is a RPC, where does it get executed?} 
Every player waits enough time to ensure availability of a segment in the server and availability the player buffer to store the current segment. This algorithm runs asynchronously whenever a player is selected as the leader. It ensure that only one player downloads a segment and other players get the segment from it. It also ensures the maximum utilization for all the players. Now, we explain the $findNextLeader()$ (leader selection) and $findCurrentQuality()$ (video quality level selection) procedures.

%A leader in a group choose next leader using $findNextLeader()$ which uses several parameters, like contributions (i.e. uploaded video data) of each player to the group $\overrightarrow{\mathcal{U}}$, qualities $\overrightarrow{\mathcal{Q}}$ of last $k$ segments, last $k$ leaders $\overrightarrow{\mathcal{P}}$, last leadership $\overrightarrow{\mathcal{L}_t}$ time for each player, expected throughput observed by each player. Similarly, the procedure $findCurrentQuality()$ uses adaptive bitrate algorithm (ABR) to find the next quality. Here we cannot use the existing ABR algorithm as that algorithm is not designed to handle the group environment. We will explain the challenges and the algorithms for this procedure.

\subsubsection{Next Leader Selection} The challenge of this procedure lies in the maintenance of the bitrate. As different players have different network quality, we need to schedule segment downloads in such a way so that every player gets enough time to download the video segment without compromising the quality, i.e.,  a player with a poor network quality should get more time to download a segment than a player with a better network quality. For example, if a player $P_1$ has twice the network bandwidth than another player $P_2$, then player $P_2$ should download one video segment from the streaming server, while the player $P_2$ should download two video segments of the same quality level within the same instance of time. Therefore, a total of three videos segments can be downloaded collectively by the two players in parallel; they can share the remaining segments with each other to render the complete video. 
%\notesc{I am not sure, does your approach ensure that players can download video segments in parallel?} \noteam{A player cannot download two segments parallelly.}

The size of the playback buffer is another challenge in the selection of the next leader to download the next video segment. Every browser-based player has a limited buffer \cite{sengupta2018hotdash}. A player cannot load a video segment if it does not have any buffer. To overcome the limited buffer issue, we use remote buffer management scheme which allows us to extend the buffer beyond its limit, as discussed later. We use Eq.~(\ref{eqn:nextLeader}) to find the next leader who will download the next video segment. Here, $IdleTime_x$ is duration from last download to now for player $P_x$. The $\mathcal{D}{q_x}$ and $\mathcal{D}{l_x}$ are the pending segment to be download queue length and status\footnote{If the segment length is $m$ bytes and $n$ bytes is downloaded, them  $\mathcal{D}{l_x} = \frac{m-n}{m}$ } of ongoing download  respectively. 
%\notesc{What do you mean by status of ongoing download? What are $q_x$ and $l_x$? Do not use the terms that have not been defined elsewhere.} \noteam{$q_x$ or $l_x$ does not mean anything individually. I have changed the symbol}
\begin{align}
	P_i &= \underset{x \in G_p}{\mathrm{argmin}} f(x) \nonumber\\
	f(x) &= IdleTime_x - \mathcal{D}{q_x} - \mathcal{D}{l_x}
	\label{eqn:nextLeader}
\end{align}

The Eqn.~\ref{eqn:nextLeader} selects the next leader who is idle for longest time (by looking at the $IdleTime_x$ parameter) among all the players in the coalition. In case all the players are busy downloading video segments, the equation consider the download load ($\mathcal{D}{q_x}$ and $\mathcal{D}{l_x}$ parameters is identify the load) on every players. The player with lowest load is selected as the next leader. Overall, the Eqn.~\ref{eqn:nextLeader} selects a player with the lightest load.

%\notesc{The intuition behind above equations should come up.}
%\noteam{We can prove that the Eqn.~\ref{eqn:nextLeader} assign furtherest segment to the slowest player.}

\subsubsection{Bitrate Selection}
%In video streaming service, bitrate selection is very tricky and challenging task. It is even trickier for collaborative environment. We follow TCP's AIMD technique as guideline to our Selection procedure. But is case of quality drop, we follow seat belt technique. We don't drop quality for any sudden change in the network. We expect it will be fixed soon, and in the mean time other player can take over and maintain the video quality without stall or quality change.
To select bitrate for segment $s_i$, we use Algorithm~\ref{algo:quality}. This algorithm suppose to execute just before a player starts downloading a segment. In the algorithm, $\varTheta$ is the measured throughput of the current player. The measured throughput has two components. The first component is the weighted throughput ($\varTheta_w$) which changes its value slowly over the time. The second component, $\varTheta_{last}$, is the throughput measured by the last finished download operation by the current player. The weighted throughput $\varTheta_{w}$ is measured as $\varTheta_{w_i} = 0.8 \times \varTheta_{w_{i-1}} + 0.2 \times \varTheta_{last}$. We use $\varTheta$ as minimum of its components because it is the worst throughput the player observed. As we use $\varTheta$ to predict the time require to download a segment, it gives us a worst time bound. $d_t$ is the time left to download the segment $s_i$ so that no player in the coalition stalls. $Cl_{i,j}$ is the content length of the $i^{th}$ segment of the quality level $j$. $m^*_i$ is the selected quality level for the $i^{th}$ segment. 
%\notesc{The algorithm is not clear. You should clearly highlight the flow and working procedure of the algorithm.}

\begin{algorithm}
	\DontPrintSemicolon
	$m^*_{n} \leftarrow m^*_{n-1}$ \; \label{algo:quality:line:mstar}
	$\varTheta \leftarrow \min(\varTheta_w, \varTheta_{last})$ \; \label{algo:quality:line:theta}
%	$t_l \leftarrow d_t - \sum_{x \in \mathcal{D}_{q}} Cl_{x,m^*_{x}} / \varTheta$\;
	$m^\prime \leftarrow \underset{m \in \mathcal{Q}}{\mathrm{argmin}} \{\left| d_t - \frac{Cl_{n,m}}{\varTheta}\right| \} $\; \label{algo:quality:line:mprime}
	\eIf{$m^\prime > m^*_{n}$}{
		\If{$m^*_{n-1} = m^*_{n-2}$}{
			$m^*_{n} \leftarrow m^*_{n-1}+1$ \label{algo:quality:line:increment}
		}
	}{
		$m^*_{n} = \lceil\frac{m^\prime + m^*_{n}}{2} \rceil$
	}
	\caption{\label{algo:quality}findCurrentQuality()}
\end{algorithm}
The Algorithm \ref{algo:quality} first select current segment quality $m^*_{n}$ same as the previous segment (in line no \ref{algo:quality:line:mstar}). However, a player needs to check if it can download this quality in time or not. So, the player find out the throughput $\varTheta$ (in line no \ref{algo:quality:line:theta}). In line \ref{algo:quality:line:mprime}, the algorithm tries to figure out the suitable quality level $m^\prime$ that can be downloaded within the time limit $d_t$. As too much fluctuation in quality level causes poor QoE, we do not change quality suddenly. We follow additive increment and multiplicative decrement. So, we increase quality level by one only if a quality chooses for at least two consecutive segments (line \ref{algo:quality:line:increment}) when network quality improves. We drop quality to average of $m^\prime$ and $m^*$. 

%The line no \ref{algo:quality:line:mprime} of algorithm \ref{algo:quality} is the place where we select the next quality. This line pick the nearest segment quality which can be download within the time limit. It can also counter little fluctuation in the measured throughput. Rest of the line the algorithm decide when to switch quality. If network quality becomes too bad, it will go down to half of the current quality level.

\subsubsection{Buffer Management}
We use a remote buffer management scheme. Here whenever a player downloads a segment, it keeps it in a buffer. Other players get the segment only when they need it. To speed up the procedure and reduce stall time, a player request a buffer as soon as they need it. If the downloader finished downloading the segment at that time, it would send the segment. However, if downloader is not yet finished the download, then it sends them as soon as it receives some part of a segment. This procedure saves transmission time between two players in case of emergency while saving excess buffer uses during normal execution.

%\subsubsection{Starvation Handling}
%Till now we have tried the minimised the stall and quality variation. However, there is a situation a group can suffer from starvation for a single player, whose network condition gone haywire for some unusual reasons. As these conditions are not possible to predict, it is impossible to come up with any Starvation avoidance algorithm. To, handle starvation, we monitor the download condition in each player. If a player finds a stalled downloading, it contacts to other player and trades the download with some other player.

