\section{(Machine) Learning-based ABR Algorithms}
We discussed various algorithms that use instantaneous parameters like buffer occupancy and throughput to decide the segment's bitrate. However, researchers have also developed different learning-based algorithms to solve the problem. In this section, we discuss some of those algorithms.

\subsection{Multi-agent Q-Learning}
Petrangeli \etal\ proposed Multi-agent Q-Learning~\cite{6838245}, a reinforcement-learning based ABR algorithm. The algorithm does not change the client-side architecture. However, it adds a co-ordinating proxy server between the server and the client. The proxy server collects and aggregates the reward from all the players. Then it computes the global signal from the reward and broadcasts the video to the players. The global signal is used to calculate the Homo Egualis~\cite{10.5555/1402298.1402344} reward, which is used to provide fairness among the players. With this Homo Egualis reward, players can compute the local reward, which leads to the selection of the video quality with the help of throughput, segment lengths, and quality levels.

\subsection{Markov Decision Process optimization}
Gadaleta \etal~\cite{8048013} and Chiariotti \etal~\cite{10.1145/2910017.2910603} formulated the adaptation problem as a Markov Decision Process (MDP)~\cite{P-1066} optimization problem. The authors have found out that the future state of the process solely depends on the present state. Here, the system's state is defined as the quality level, available throughput, size of the segment, and the playback buffer. The state transition happens based on the action taken in the current state, essentially the bitrate. The reward is calculated based on the previous state, action taken in the previous state, and the current state. Chiariotti \etal\ used online reinforcement learning to solve the problem to make the best possible decision, while Gadaleta \etal\ used Q-Learning-based deep-neural network to solve the same problem.

\subsection{Pensieve}
Mao \etal\ designed Pensieve~\cite{mao2017neural} to solve the bitrate selection problem using Recurrent Neural Network. Pensieve treats multiple parameters, including buffer occupancy, download history, playback time as the current state, and bitrate selection as the action to move to the next state. Pensieve uses the QoE as the reward for the state transition. The aim here is to increase the reward, i.e., QoE, by taking accurate action. Pensieve selects the action based on the policy, which is a probability distribution of state and action pairs. As there are intractably many such pairs that exist, Pensieve uses a neural network to represent the policy with manageable parameters. It uses the actor-critic neural-network to train the policy with the policy gradient method~\cite{sutton1999policy}. It also uses the A3C~\cite{10.5555/3045390.3045594} algorithm, an actor-critic method involving two networks, to train its model.\\
Pensieve proposes a simulation-based method to train the network first, and then the trained model is used in the real playback system. To make the training process even faster, they propose asynchronous parallel training involving primary and secondary networks.

\subsection{Auto-tuning Video ABR Algorithm (Oboe)}
Oboe~\cite{Akhtar2018} is another learning-based ABR technology devised by Akhtar \etal. While Oboe does not directly provide any algorithm to select bitrate for each segment, it tunes the configuration of other ABR algorithms, including MPC~\cite{yin2015control,10.1145/2670518.2673877}, BOLA~\cite{Spiteri2016} and Penseive~\cite{mao2017neural} based on the network state. Oboe precomputes the appropriate configuration settings for any algorithm based on the different network states using reinforcement learning. It applies the learned model online to tune the configuration to provide the ABR algorithm's best possible outcome.


\subsection{Periodical Experience Replay for Multi-path (PERM)}
PERM~\cite{9155492}, developed by Guan \etal, is an actor-critic network-based neural adaptive video streaming system that can exploit the multipath capability of multi-homing devices. PERM uses one actor-network to decide the quality for a segment and another for determining the path to fetch that segment and uses only one critic-network to evaluate the decision.

\subsection{Super-Resolution based video streaming}
Zhang \etal\ presented a novel approach to increase video streaming quality in their paper \cite{9155384} by improving video quality by the technique call Super-Resolution. Video super-resolution is a technique to reconstruct high-resolution frames from consecutive low-resolution frames. They use a reinforcement learning (RL)-based decision engine to decide the download resolution and whether reconstruction is required or not. The RL-based decision engine is designed based on the state of the actor-critic network.

\subsection{LiveClip}
Most of the ABR algorithms improve video quality for a long video, and they start with the lowest possible video quality. Although it is okay for long videos as the initial part usually not that important, it is a problem for short videos. There are services to stream concise videos. LiveClip~\cite{10.1145/3386290.3396937} solves the problem by designing a deep reinforcement learning-based ABR algorithm for short videos. He \etal\ consider a particular scenario where the user can scroll through the page to change the video, and the video needs to play whenever it in the visual section. They use adaptive streaming for the entire session instead of per video and adaptation based on the application session (i.e., multiple videos in a session).

\subsection{Overhead-aware Adaptive Video Streaming (Grad)}
Grad~\cite{10.1145/3394171.3413512} is an overhead-award ABR for SVC-based video streaming. By default, DASH is not designed for SVC-based video streaming, so most of the ABR algorithms do not support DASH directly. Also, SVC involves much extra overhead in decoding. Liu \etal\ design grad to mitigate those problems. To reduce decoding overhead, they propose {\tt jump enabled hybrid coding} where only one enhancement layer can be used to jump multiple SVC layers. On top of this optimization, they use actor-critic network-based reinforce-learning to adapt the quality.

\subsection{Summary}
Machine learning and deep learning can perform better than classical systems if trained correctly. The ABR algorithms, like Pensieve, Oboe, etc., show the potential of the machine learning algorithms. The idea of super-resolution and liveClip gives another perspective to the video streaming paradigm. While all these ABR algorithms work well in the evaluation, these are not easy to deploy widely. Most of the ABR algorithm needs to train for a long time before they can be used. The training time is enormous and computationally heavy. Training needs to be done frequently and sometimes, per video. Another big hurdle in the deployment of these algorithms is their library dependency. To infer the correct decision from a pre-trained model, an ML algorithm needs to run in the player. Library supports are essential to run ML algorithms in players, and not all the different platforms support all the libraries. Inference can be made by offloading computation to a server. However, it is incredibly inefficient as it involves network delay. We feel there is much work needed before any of the algorithms can be deployed widely and efficiently.

\begin{table}[h!]
	\resizebox{\textwidth}{!}{
		\begin{tabular}{|l|cccc|c|c|l|l}
			\hline
			\multicolumn{1}{|c|}{\multirow{3}{*}{Algorithm}} & \multicolumn{4}{c|}{Goal involved} & \multirow{3}{*}{\begin{turn}{90}No Extra Module\end{turn}} & \multirow{3}{*}{\begin{turn}{90}Continuous Traffic\end{turn}} & \multicolumn{1}{c|}{\multirow{3}{*}{Remark}} \\
			\cline{2-5}
			& \begin{turn}{90}Rebuffering\end{turn} & \begin{turn}{90}Quality\end{turn} & \begin{turn}{90}Smoothness\end{turn} & \begin{turn}{90}Fairness\end{turn} & & & \\
			& & & & & & & \\
			\hline
			Multi-agent Q-Learning\cite{6838245} & \green{\cmark} & \green{\cmark} & \green{\cmark} & \green{\cmark} & \red{\xmark} & \red{\xmark} & \begin{tabular}[c]{@{}l@{}}Reinforcement Learning\\ Coordination proxy required\end{tabular} \\
			RL-based Online Learning\cite{10.1145/2910017.2910603} & \green{\cmark} & \green{\cmark} & \green{\cmark} & \red{\xmark} & \green{\cmark} & \red{\xmark} & \begin{tabular}[c]{@{}l@{}}Markov decision process\\ Reinforcement learning to solve MDP\end{tabular} \\
			D-DASH \cite{8048013}& \green{\cmark} & \green{\cmark} & \green{\cmark} & \red{\xmark} & \green{\cmark} & \red{\xmark} & \begin{tabular}[c]{@{}l@{}}Markov decision process\\ Deep neural network solve MDP\end{tabular} \\
			Pensieve\cite{mao2017neural} & \green{\cmark} & \green{\cmark} & \green{\cmark} & \red{\xmark} & \green{\cmark} & \red{\xmark} & \begin{tabular}[c]{@{}l@{}}Policy gradient training\\ A3C based actor-critic network\\ Simulation based training\end{tabular} \\
			Oboe\cite{Akhtar2018} & \green{\cmark} & \green{\cmark} & \green{\cmark} & \red{\xmark} & \green{\cmark} & \red{\xmark} & Configuration tuning \\
			PERM\cite{9155492} & \green{\cmark} & \green{\cmark} & \green{\cmark} & \red{\xmark} & \green{\cmark} & \red{\xmark} & \begin{tabular}[c]{@{}l@{}}Path selection problem\\ Actor-critic network with two actor network\end{tabular} \\
			Super-resolution\cite{9155384} & \green{\cmark} & \green{\cmark} & \green{\cmark} & \red{\xmark} & \red{\xmark} & \red{\xmark} & \begin{tabular}[c]{@{}l@{}}Post download quality enhancement\\ Neural network based solution\end{tabular} \\
			\hline
		\end{tabular}
	}
	\caption{\label{chapter02:table:comparison_ml}Comparisons of machine-learning based ABR algorithms}
\end{table}
